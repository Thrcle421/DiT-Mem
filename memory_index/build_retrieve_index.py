"""
Build retrieval index for text-to-text video retrieval.

This script constructs a FAISS index from text embeddings generated by a transformer model,
enabling efficient similarity search over video captions.

Supports multi-GPU parallel processing for faster embedding generation.
"""

import faiss
import torch
import pandas as pd
import numpy as np
import json
import os
from transformers import AutoTokenizer, AutoModel
import torch.nn.functional as F
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import math

# Configuration
MODEL_PATH = 'model/gte-base-en-v1.5'
CSV_PATH = 'data/memory.csv'
VIDEO_DIR_BASE = 'video'
OUTPUT_INDEX_PATH = 'memory_index/labels.index'
OUTPUT_ID_MAP_PATH = 'memory_index/id_map.json'
BATCH_SIZE = 256
NUM_GPUS = 8  # Number of GPUs to use

# Load and preprocess data
print("Loading dataset...")
df = pd.read_csv(CSV_PATH)
df = df.dropna(subset=['caption'])
df = df[df['caption'].str.strip() != '']

labels = df['caption'].tolist()
video_ids = df['video'].tolist()
print(f"Loaded {len(labels):,} captions")

# Detect available GPUs
if torch.cuda.is_available():
    num_gpus = min(NUM_GPUS, torch.cuda.device_count())
    gpu_ids = list(range(num_gpus))
    print(f"Using {num_gpus} GPU(s): {gpu_ids}")
else:
    num_gpus = 0
    gpu_ids = []
    print("CUDA not available, using CPU")

# Initialize encoder model
print(f"Initializing encoder: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

# Initialize models on each GPU
models = []
if num_gpus > 0:
    for gpu_id in gpu_ids:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True).to(f'cuda:{gpu_id}')
        model.eval()
        models.append(model)
    print(f"Loaded {num_gpus} model copies on GPUs")
else:
    model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model.eval()
    models = [model]

def process_batch_on_gpu(batch_data, gpu_id, model_idx):
    """Process a batch on a specific GPU"""
    batch_labels, batch_indices = batch_data
    device = f'cuda:{gpu_id}' if num_gpus > 0 else 'cpu'
    model = models[model_idx]
    
    batch_dict = tokenizer(
        batch_labels, 
        max_length=512, 
        padding=True, 
        truncation=True, 
        return_tensors='pt'
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**batch_dict)
        embeddings = outputs.last_hidden_state[:, 0]
        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)
    
    return normalized_embeddings.cpu().numpy(), batch_indices

# Generate text embeddings with multi-GPU parallel processing
print("Computing embeddings...")
num_batches = math.ceil(len(labels) / BATCH_SIZE)

# Process batches in parallel across GPUs
if num_gpus > 1:
    # Multi-GPU parallel processing: distribute batches across GPUs
    all_embeddings_list = []
    batch_indices_list = []
    
    with ThreadPoolExecutor(max_workers=num_gpus) as executor:
        futures = []
        for batch_idx in range(num_batches):
            start_idx = batch_idx * BATCH_SIZE
            end_idx = min(start_idx + BATCH_SIZE, len(labels))
            batch_labels = labels[start_idx:end_idx]
            batch_indices = list(range(start_idx, end_idx))
            batch_data = (batch_labels, batch_indices)
            
            # Assign to GPU in round-robin fashion
            gpu_id = gpu_ids[batch_idx % num_gpus]
            model_idx = batch_idx % num_gpus
            future = executor.submit(process_batch_on_gpu, batch_data, gpu_id, model_idx)
            futures.append((future, batch_idx))
        
        # Collect results and maintain order
        results = [None] * num_batches
        for future, batch_idx in tqdm(futures, desc="Encoding", unit="batch"):
            embeddings_np, batch_indices = future.result()
            results[batch_idx] = (embeddings_np, batch_indices)
        
        # Concatenate in order
        for batch_idx in range(num_batches):
            embeddings_np, batch_indices = results[batch_idx]
            all_embeddings_list.append(embeddings_np)
    
    embeddings_np = np.concatenate(all_embeddings_list, axis=0).astype('float32')
else:
    # Single GPU or CPU processing
    if num_gpus > 0:
        device = f'cuda:{gpu_ids[0]}'
    else:
        device = 'cpu'
    model = models[0]
    all_embeddings_list = []
    for i in tqdm(range(0, len(labels), BATCH_SIZE), desc="Encoding", unit="batch"):
        batch_labels = labels[i:i+BATCH_SIZE]
        batch_dict = tokenizer(
            batch_labels, 
            max_length=512, 
            padding=True, 
            truncation=True, 
            return_tensors='pt'
        ).to(device)
        with torch.no_grad():
            outputs = model(**batch_dict)
            embeddings = outputs.last_hidden_state[:, 0]
            normalized_embeddings = F.normalize(embeddings, p=2, dim=1)
            all_embeddings_list.append(normalized_embeddings.cpu().numpy())
    
    embeddings_np = np.concatenate(all_embeddings_list, axis=0).astype('float32')
embedding_dim = embeddings_np.shape[1]
print(f"Embeddings: {embeddings_np.shape} (dim={embedding_dim})")

# Build FAISS index
print("Building FAISS index...")
# IndexFlatIP computes inner product on normalized vectors (equivalent to cosine similarity)
cpu_index = faiss.IndexFlatIP(embedding_dim)

# Check if GPU version of FAISS is available
has_faiss_gpu = hasattr(faiss, 'StandardGpuResources')

if num_gpus > 0 and has_faiss_gpu:
    # Use GPU 0 for FAISS index construction (can be changed if needed)
    try:
        res = faiss.StandardGpuResources()
        gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
        gpu_index.add(embeddings_np)
        cpu_index = faiss.index_gpu_to_cpu(gpu_index)
        print("FAISS index built on GPU")
    except Exception as e:
        print(f"Warning: Failed to use GPU for FAISS index, falling back to CPU: {e}")
        cpu_index.add(embeddings_np)
elif num_gpus > 0 and not has_faiss_gpu:
    print("Warning: faiss-gpu not installed, using CPU for index construction")
    print("To use GPU acceleration, install faiss-gpu: pip install faiss-gpu")
    cpu_index.add(embeddings_np)
else:
    cpu_index.add(embeddings_np)
    
print(f"Index constructed: {cpu_index.ntotal:,} vectors")

# Save index and metadata
print("Saving outputs...")
os.makedirs(os.path.dirname(OUTPUT_INDEX_PATH) if os.path.dirname(OUTPUT_INDEX_PATH) else '.', exist_ok=True)
os.makedirs(os.path.dirname(OUTPUT_ID_MAP_PATH) if os.path.dirname(OUTPUT_ID_MAP_PATH) else '.', exist_ok=True)

faiss.write_index(cpu_index, OUTPUT_INDEX_PATH)
print(f"  Index: {OUTPUT_INDEX_PATH}")

id_map = {i: video_id for i, video_id in enumerate(video_ids)}
with open(OUTPUT_ID_MAP_PATH, 'w') as f:
    json.dump(id_map, f, indent=2)
print(f"  ID map: {OUTPUT_ID_MAP_PATH}")

metadata_path = OUTPUT_ID_MAP_PATH.replace('id_map.json', 'metadata.json')
metadata = []
for i, (video_id, caption) in enumerate(tqdm(zip(video_ids, labels), desc="Extracting metadata", unit="item")):
    row = df[df['video'] == video_id].iloc[0]
    metadata.append({
        'index': i,
        'video': video_id,
        'caption': caption,
        'aesthetic_score': float(row.get('aesthetic score', 0)),
        'motion_score': float(row.get('motion score', 0)),
        'temporal_consistency_score': float(row.get('temporal consistency score', 0)),
        'camera_motion': row.get('camera motion', ''),
        'frame': int(row.get('frame', 0)),
        'fps': float(row.get('fps', 0)),
        'seconds': float(row.get('seconds', 0))
    })
with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=2)
print(f"  Metadata: {metadata_path}")

print("Complete.")
